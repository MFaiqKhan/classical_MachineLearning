{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG0wSBdxyjJX13/ctU16br",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MFaiqKhan/classical_MachineLearning/blob/main/k_Fold_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which is Model is better for the same dataset? Cross validation is the technique to answer that question.\n",
        "\n",
        "Cross-Validation can be performed as:\n",
        "\n",
        "Option 1: \n",
        "Use All available data for training and test on same dataset(Using 100% to train model and train model)\n",
        "\n",
        "Option 2: Split dataset for training and testing( train_test_split method )\n",
        "\n",
        "Above option ain't that good thats why we use K-fold validation\n",
        "\n",
        "K_fold : divide dataset samples into folds, 100 samples divided in 5 folds where 4 will be used for train and one will be used for test one by one .\n",
        "then we will average the model score. \n",
        "\n",
        "K-fold cross-validation is a technique used to evaluate the performance of a machine learning model. It involves splitting the data into K equal parts or \"folds\" and training the model K times. In each iteration, one of the folds is used as the validation set and the remaining K-1 folds are used for training. The process is repeated K times, with each of the K folds used exactly once for validation.\n",
        "\n",
        "The results of each fold are then averaged to obtain a single estimate of the model's performance. K-fold cross-validation helps to mitigate the issue of overfitting by using different training and validation sets in each iteration, ensuring that the model is evaluated on a variety of data.\n",
        "\n",
        "Common values for K include 5, 10, and 20, but the appropriate value will depend on the size of the dataset and the computational resources available. A larger value of K will result in a more accurate estimate of the model's performance, but it will also increase the computational time.\n",
        "\n"
      ],
      "metadata": {
        "id": "nJJC-ZEY58Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()"
      ],
      "metadata": {
        "id": "SiUrR5WE96kB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size = 0.3)\n",
        "\n"
      ],
      "metadata": {
        "id": "tNIx8IQfDuMi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the sample distribution in splits are not uniform , means they will change again if we run the above code again.\n",
        "If I run the code above multiple times it will give me different results with variation in score accuracy ."
      ],
      "metadata": {
        "id": "zUyv9NHeU-Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "lr.score(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKgH-m7rH6Ak",
        "outputId": "06ef0737-ab7e-49cd-864a-e0691e0d7684"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9555555555555556"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "svm.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HZV-o7iQuqN",
        "outputId": "fe1022bf-3197-4514-911e-713ecc474597"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.987037037037037"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "rf.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBq3QE1qROSj",
        "outputId": "aa01416f-4831-4b23-c936-0cb795cca812"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.975925925925926"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using K-fold,\n",
        "\n",
        "We set shuffle=True to shuffle the data before splitting\n",
        "\n",
        "The random_state parameter in the KFold method of scikit-learn is used to set the random seed used by the random number generator for shuffling the data samples. It is an optional parameter and if not specified, it will use a different random seed each time it is called.\n",
        "\n",
        "Setting the random_state parameter to a fixed value ensures that the same samples are selected for each fold when the KFold method is called again with the same parameters. This is useful for reproducibility purposes and allows for the results to be compared across multiple runs.\n",
        "\n",
        "For example, if you set random_state=42 when creating a KFold object, it will always generate the same sequence of random numbers for shuffling the data samples, and hence, will produce the same folds for cross-validation every time it is called with the same parameters."
      ],
      "metadata": {
        "id": "II89fIApVebJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=3) # means sample is divide into 3 folds, one is for validation other two for training\n",
        "kf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePaLWfXrVgjZ",
        "outputId": "28f77d13-c74b-411f-d513-7e2d8725bb1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KFold(n_splits=3, random_state=None, shuffle=False)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method kf.split([1,2,3,4,5,6,7,8,9]) is used to generate the indices for splitting the data [1,2,3,4,5,6,7,8,9] into training and testing sets using the KFold cross-validation with the number of folds specified when creating the KFold object.\n",
        "\n",
        "The resulting train_index and test_index variables in the example will contain the indices of the data points for the current fold."
      ],
      "metadata": {
        "id": "p6_hhQOXJmiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n",
        "  print(\"Train Index:\", train_index, \", Test Index:\", test_index) # gets me the index for datapoints of 3 folds of training and test samples in each fold."
      ],
      "metadata": {
        "id": "5pOwIy-pigAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51aec67-fdfc-4c2a-8d87-18912509b957"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Index: [3 4 5 6 7 8] , Test Index: [0 1 2]\n",
            "Train Index: [0 1 2 6 7 8] , Test Index: [3 4 5]\n",
            "Train Index: [0 1 2 3 4 5] , Test Index: [6 7 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the code shows the indices of the training and test sets for each fold generated by the KFold cross-validation method.\n",
        "\n",
        "In each iteration of the for loop, the code prints the indices of the training and test sets for the current fold. The output is as follows:\n",
        "\n",
        "In the first iteration, the training set includes the samples with indices 3, 4, 5, 6, 7, and 8, while the test set includes the samples with indices 0, 1, and 2.\n",
        "In the second iteration, the training set includes the samples with indices 0, 1, 2, 6, 7, and 8, while the test set includes the samples with indices 3, 4, and 5.\n",
        "In the third iteration, the training set includes the samples with indices 0, 1, 2, 3, 4, and 5, while the test set includes the samples with indices 6, 7, and 8.\n",
        "In summary, the KFold method is splitting the data into 3 folds, and each fold is used once as a test set while the remaining folds are used as training sets. By shuffling the data and specifying a random state, the KFold method ensures that the data is randomly split into the folds for cross-validation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QVYYsE8tY-iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_score(model, X_train, X_test, y_train, y_test):\n",
        "  model.fit(X_train, y_train)\n",
        "  return model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "SXHCmiW18ZeU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function first fits the model to the training data using the model.fit() method, which trains the model on the input data. It then computes the accuracy score of the model on the testing data using the model.score() method, which returns the mean accuracy on the given test data and labels.\n",
        "\n",
        "Finally, the function returns the accuracy score obtained by the model on the testing data. This function can be useful when we want to evaluate different machine learning models on the same dataset using k-fold cross-validation or any other evaluation method."
      ],
      "metadata": {
        "id": "ZBTVPx5IOm3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_score(SVC(), X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yAiq5OaPwH4",
        "outputId": "6ff96d7c-912b-4708-dd39-dadf6deb0d2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.987037037037037"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several types of k-folds:\n",
        "\n",
        "- KFold: This is the basic K-fold cross-validation. In this, the data is split into K folds, and each fold is used as a testing set at some point.\n",
        "\n",
        "- StratifiedKFold: This K-fold cross-validation is suitable for classification problems. Each fold is made by preserving the percentage of samples for each class.\n",
        "\n",
        "- TimeSeriesSplit: This K-fold cross-validation is suitable for time-series data. In this, the data is split in such a way that each fold contains data from earlier time points than the previous fold.\n",
        " \n",
        "- GroupKFold: This K-fold cross-validation is suitable for when the data contains groups. In this, the data is split in such a way that the same group does not appear in two different folds.\n",
        "\n",
        "Stratified K-Fold: Stratified K-Fold cross-validation is a modification of KFold. The main difference between these two is that in Stratified K-Fold, we divide the data into folds, so that each fold has the same proportion of target values as the complete dataset. This method helps in obtaining better results for classification tasks. Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. For example, in a binary classification problem, each fold should have approximately the same percentage of samples of each target class as the complete dataset."
      ],
      "metadata": {
        "id": "GOn6BONnRlm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stratifiedKFold divides classification in uniform manner, \n",
        "e.g: types of number of flowers in one fold and other fold should be same.\n",
        "\n",
        "this means that each fold contains roughly the same proportions of the two types of class labels."
      ],
      "metadata": {
        "id": "nNStU3yHSJy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "folds = StratifiedKFold(n_splits = 3)\n",
        "folds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjz-quDXRrZT",
        "outputId": "8854c704-a001-4373-af6e-df55198f717b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StratifiedKFold(n_splits=3, random_state=None, shuffle=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_l = []\n",
        "scores_svm = []\n",
        "scores_rf = []\n",
        "\n",
        "for train_index, test_index in kf.split(digits.data, digits.target):\n",
        "  #print(\"Train Index:\", train_index, \", Test Index:\", test_index)\n",
        "  # loop over the splits and fit a model on each fold\n",
        "    X_train, X_test = digits.data[train_index], digits.data[test_index]\n",
        "    y_train, y_test = digits.target[train_index], digits.target[test_index]\n",
        "\n",
        "    scores_l.append(get_score(LogisticRegression(), X_train, X_test, y_train, y_test))\n",
        "    scores_svm.append(get_score(SVC(), X_train, X_test, y_train, y_test))\n",
        "    scores_rf.append(get_score(RandomForestClassifier(), X_train, X_test, y_train, y_test))"
      ],
      "metadata": {
        "id": "3bwo4jIKTkbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is performing K-fold cross-validation for classification on the digits dataset using Logistic Regression, Support Vector Machine (SVM), and Random Forest Classifier models.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "- Three empty lists, scores_l, scores_svm, and scores_rf, are created to store the accuracy scores of each fold for each model.\n",
        "\n",
        "- The KFold iterator is instantiated with 3 folds using kf.split(digits.data, digits.target). It returns the indices of the data for each train-test split.\n",
        "- For each split, X_train, X_test, y_train, and y_test are obtained from the indices returned by the KFold iterator.\n",
        "- The get_score() function is called for each model, passing the training and testing data for the current split. The function fits the model on the training data and returns the accuracy score on the testing data.\n",
        "- The accuracy score for each model is appended to the corresponding list.\n",
        "- After all the splits have been processed, the scores_l, scores_svm, and scores_rf lists contain the accuracy scores for each model for each fold.\n",
        "\n",
        "The accuracy scores can then be used to compare the performance of the three models and determine which one works best for the given classification problem."
      ],
      "metadata": {
        "id": "FxfUPvkXabxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_l, scores_svm, scores_svm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onFktdxCXWJW",
        "outputId": "2bff3b2c-93f2-42b7-aab3-937d1ce1b539"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.9232053422370617, 0.9415692821368948, 0.9148580968280468],\n",
              " [0.9666110183639399, 0.9816360601001669, 0.9549248747913188],\n",
              " [0.9666110183639399, 0.9816360601001669, 0.9549248747913188])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "metadata": {
        "id": "WdiGH8rPY0n6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`cross_val_score` is a function in the `sklearn.model_selection` module that provides a simple way to perform cross-validation and obtain scores for a given model and dataset. It takes as input a machine learning model, a dataset (features and target), and the number of folds to use for cross-validation. It then splits the dataset into the specified number of folds, trains the model on each fold while using the remaining folds as validation data, and returns the evaluation score for each fold.\n",
        "\n",
        "The main advantage of using cross_val_score is that it simplifies the process of cross-validation, making it easier to compare the performance of different models or hyperparameters. Additionally, it provides an efficient way to obtain a reliable estimate of the model's performance on unseen data, by testing the model on multiple different test sets.\n",
        "\n",
        "The function returns an array of scores (one for each fold) which can be used to calculate the mean and standard deviation of the performance of the model across all the folds."
      ],
      "metadata": {
        "id": "xqRacyUzeBZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "hQk4XmuldgyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Logistic_scores = cross_val_score(LogisticRegression(), digits.data, digits.target)\n",
        "Logistic_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i55cznuY6OC",
        "outputId": "d18b1435-8b7c-4e6a-c8ce-8b1fa52277c8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92222222, 0.86944444, 0.94150418, 0.93871866, 0.89693593])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(Logistic_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ckAhohiczZD",
        "outputId": "21df778c-679d-4ce2-9b60-f8b0aea5f4e9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9137650882079852"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Machines (SVM)**"
      ],
      "metadata": {
        "id": "EVS8Y31_dcNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_scores = cross_val_score(SVC(), digits.data, digits.target)\n",
        "SVM_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRqamlg1bpYc",
        "outputId": "b5acadb0-012c-4b65-a14f-5d89f09e8a0f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.96111111, 0.94444444, 0.98328691, 0.98885794, 0.93871866])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(SVM_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozs7Pcsvc-6u",
        "outputId": "16a86f4b-2604-4ca5-ee9d-31f9cae61275"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9632838130609718"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random** **Forest**"
      ],
      "metadata": {
        "id": "4NobE1WedXgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFC_scores = cross_val_score(RandomForestClassifier(), digits.data, digits.target)\n",
        "RFC_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2vzFzFGbprk",
        "outputId": "a7baca0c-903d-4808-d504-de146f19aa0b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.93611111, 0.90833333, 0.95543175, 0.97214485, 0.9275766 ])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(RFC_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we3GnmfHdOUd",
        "outputId": "569bc449-c8e8-4995-af06-91a94dfc3974"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9399195295574125"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best Performing is SVM with : 0.9632838130609718**"
      ],
      "metadata": {
        "id": "yfRRPfQWdlBe"
      }
    }
  ]
}